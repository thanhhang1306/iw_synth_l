variant_name,experiment,rank,weighted_f1,macro_f1,accuracy,roc_auc,cohen_kappa
Back-translation baseline,moderate_aug,best,0.7965588630822209,0.7885198498148753,0.7975,0.9037642867549532,0.6877650142625857
Back-translation baseline,synthetic_only,worst,0.7527346050402198,0.7462759472802077,0.7525,0.8828227768697409,0.620034542314335
Base LLM + Lexicon,real_only,best,0.7672875035610578,0.7570700903171254,0.765,0.8919850921655857,0.6422659029931688
Base LLM + Lexicon,synthetic_only,worst,0.4791323955608962,0.4754836125444734,0.49,0.6833102055021394,0.23005066945962
Base LLM + MT,low_aug,best,0.7823094268915165,0.7697152764316945,0.785,0.9120075870480248,0.6666537462692352
Base LLM + MT,synthetic_only,worst,0.5125254946165656,0.4990970325127701,0.4975,0.7198014298286588,0.2663430302587876
In-language LLM (mT5-LR),full_aug,best,0.796397734786862,0.7875216640398075,0.795,0.9138055014375251,0.6860973672373697
In-language LLM (mT5-LR),synthetic_only,worst,0.5299454770324177,0.5088103851059588,0.535,0.6655897353687767,0.2787762461466875
Seeds + LLM + Lex,low_aug,best,0.7735780810769289,0.7628449235839003,0.7725,0.9031732927252026,0.6502690238278248
Seeds + LLM + Lex,synthetic_only,worst,0.4094188479390442,0.416946207261666,0.4225,0.6088755725739233,0.1610220277116968
Seeds + LLM + MT,full_aug,best,0.7832916651613135,0.7756060551320495,0.785,0.9060242836354184,0.6685742913848585
Seeds + LLM + MT,synthetic_only,worst,0.6134801876955162,0.6065345846367745,0.6075,0.7968117431848376,0.4248662905707377
V5 + QA,full_aug,best,0.7797026993713793,0.7776380842817426,0.78,0.9014623658329756,0.6666856049845653
V5 + QA,synthetic_only,worst,0.5016344433741571,0.5082101828392473,0.53,0.71609685762707,0.3063242565124344
V6 + QA,low_aug,best,0.7843525282143472,0.775527490938578,0.7825,0.9050830671454406,0.6674502608795367
V6 + QA,synthetic_only,worst,0.6272977072887145,0.6086295946407857,0.625,0.7696694749949863,0.427852156997368
V5 + CoT,moderate_aug,best,0.7859561476483186,0.7791998959258746,0.785,0.8971685315284224,0.6703749484960857
V5 + CoT,synthetic_only,worst,0.4315282492157872,0.4255440069499716,0.4325,0.6233055979284647,0.1502900991952086
V6 + CoT,low_aug,best,0.8111825384149447,0.8034799325282304,0.81,0.91798263332621,0.7101836139340667
V6 + CoT,synthetic_only,worst,0.4452834529724697,0.4498937038725034,0.485,0.7297701618134197,0.2550402314438115
V7 + CoT (full Lex path),full_aug,best,0.7748625631603123,0.7655945150478912,0.775,0.8893156432288708,0.6551691108152378
V7 + CoT (full Lex path),synthetic_only,worst,0.5565551289385673,0.5470158813980325,0.5575,0.7155252212286082,0.3200284281900079
V8 + CoT (full MT path),full_aug,best,0.8049599427168576,0.7977141298417894,0.805,0.9233093958420796,0.7009833049011903
V8 + CoT (full MT path),synthetic_only,worst,0.5315020898641588,0.5260652502031812,0.5375,0.7551779966810064,0.3169840229641048
